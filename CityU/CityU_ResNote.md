# CityU_ResNote

2021.05.12-Unknown

Hongkong,China

siyuexi@hust.edu.cn

## Knowledge



### 正交矩阵

即满足$A^TA=I$的矩阵。若矩阵$A=[a_1,a_2,...,a_i]$且$a_i$为各个列向量，则矩阵乘积展开后可以得到如下关系：
$$
a_ia_j=\begin{cases}
1,&i=j\\
0,&i\neq j
\end{cases}
$$
即矩阵内各向量之间相互正交。

对于正交矩阵，组成它的列向量构成了一个空间的基，称之为：规范正交基。 对于一个空间而言，可以找到很多个不同的基来表示的(参考相似矩阵的基底变换)。因此可以找到一组基础变换$P_1P_2...P_n$使得该空间的正交基被变换到各个坐标轴上，此时矩阵为对角矩阵。

可以归纳为：**正交矩阵一定可以转换为对角矩阵**



### 矩阵分解

$A$为$n$阶方阵，若数$λ$和$n$维非0列向量$x$满足
$$
Ax=\lambda x
$$
那么数$λ$称为$A$的特征值，$x$称为$A$的对应于特征值$λ$的特征向量。

式$Ax=λx$也可写成
$$
(A-\lambda E)x=0
$$
并且$|λE-A|$即为$A$ 的特征多项式。当特征多项式等于0的时候，称为$A$的特征方程，特征方程是一个齐次线性方程组，求解特征值的过程其实就是求解特征方程的解。

求解特征多项式=0的方程并得到特征向量、特征值，矩阵$A$可以被分解为以下形式：
$$
A=W\Sigma W^{-1}
$$
其中$W$是特征向量构成的特征矩阵，$\Sigma$是特征值组成的对角矩阵。

把$W$中的向量进行标准化处理，即使得$w_i^tw_i=1$。设矩阵具有不同特征值，即特征向量相互正交（对于鲁棒的图结构表示矩阵而言都是满足的），那么这样便得到了**标准正交化**的一组基$W$。由正交矩阵的定义可得：
$$
A=W\Sigma W^T
$$
这样方阵$A$就完成了**特征值矩阵分解**。特征值矩阵的分解在数据压缩领域有着及其重要的应用。如长宽相等的灰度图像可以直接进行特征值分解，再将$\Sigma$中的特征值排序，去掉较小的一半特征值（置零），再进行反变换，其图像只是稍微变得模糊，肉眼感知上差距不明显。

但是在大多数情况下，矩阵$A$并非是方阵。因此不能进行特征值分解。可以采用另一种思路分解：
$$
A=U\Sigma V^T
$$
这里设$A$是一个$m\times n$大小的矩阵，那么$U$是$m\times m$大小的方阵，$V$是$n\times n$大小的方阵。

由于$AA^T$与$A^TA$分别和$U$，$V$矩阵有相同的维度。由于他们都是方阵，可以认为$U$，$V$即为$AA^T$与$A^TA$的特征向量所构成的矩阵（证明略）。
$$
Uu_i=(AA^T)u_i=\lambda_i u_i\\
Vv_i=(A^TA)v_i=\lambda_i v_i\\
$$
这里把$AA^T$的每个特征向量$u_i$称为$A$的左奇异向量，$A^TA$的每个特征向量$v_i$称为$A$的右奇异向量。

$A$的奇异值矩阵$\Sigma$可以通过下式求得：
$$
A=U\Sigma V^T \Rightarrow AV=U\Sigma \Rightarrow Av_i=\sigma_i u_i \Rightarrow \sigma _i=\frac{A v_i}{u_i}
$$
这就是**奇异值分解**。奇异值分解也广泛适用于数据降维任务，如PCA（主成分分析）中对数据间的协方差矩阵进行降维操作，原理同图像压缩算法一致。



### 图网络模型简述

长期以来，深度学习在CV领域的模型（如CNN）基本都建立在数字图像处理的传统理论之上。其基本的数据降维操作——卷积，本质是模板（卷积核）在**图像（image）**每个像素的**邻域**上进行线性运算的过程。这里的邻域即为矩形图像的欧氏空间邻域。

但扩展到例如小样本学习等场景下时，模型所重视的不再是每个图像本身的信息，而是图像（或者一些其他任务）之间的**关联**。这就意味着每个结点可能不再仅仅与“周围”的结点有关。“关系网络”可能会拥有更加复杂的拓扑结构，而非像图像一样依旧保持着规则的8邻域结构。鉴于小样本学习的结点网络可能存在于非欧空间中，因此使用**图（graph）**会更加实用：图之间的关系高度自定义化，且不规则。图网络使得关系网络错综复杂的事务有了更直观具象化的模型表示。

但是更自定义化的关系也带来了表示和计算的复杂度：

1. 图网络需要由邻接矩阵定义拓扑结构，而图像则不需要。前者意味着更大的空间开销以及更多的读写时间开销。
2. 原先定义在二维图像上的卷积操作具有严格的数学意义，并且易于计算。然而卷积操作在图上变得难以实现，需要基于邻接矩阵的分解来计算，并且不再具有和一维连续函数的卷积运算严格相同的数学意义。



### 图网络模型的基本计算

GCN（默认为无向图）的拓扑结构常常被表示为**正则化拉普拉斯矩阵**而非一般性的邻接矩阵。这样能带来更多的信息（如度矩阵），并且易于计算（正交化）：
$$
L=I_n-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$
其中，$A$为邻接矩阵，$D$为度矩阵

正则化拉普拉斯矩阵具有半正定性质，因此可以进行矩阵分解：
$$
L=U\Lambda U^{-1}=U\Lambda U^T
$$
其中$U$是**标准正交化**的特征矩阵，由标准正交化的特征向量组成。标准正交化使得矩阵$U$的逆即为其转置，减少了计算开销；$\Lambda$即与$U$对应的特征值对角矩阵。
$$
U=[u_0,u_1,...,u_n]\\
\Lambda=\left[ \begin{matrix} \lambda_0 & 0 & 0 & ... & 0\\ 0 & \lambda_1 & 0 & ... & 0 \\ : & : & : &  & : \\ 0 & 0 & 0 & ... & \lambda_n \end{matrix} \right]
$$
图信号被定义为：
$$
x\in R^N
$$
是一个由图中各个结点组成的一个特征向量。$x_i$是其中第$i$个结点的信号值。

定义了图的信号，便可以对信号作傅里叶变换。图的傅里叶变换被定义为：
$$
\mathcal{F}(x)=U^T\hat{x}
$$
其意义是把特征向量的每个维度通过线性变化映射到另一组**正交基**所表示的空间中。符合傅里叶变换的本质（但仍然与一维连续信号的傅里叶变换定义不严格相同）。

根据傅里叶反变换的本质，图的傅里叶反变换即为：
$$
\mathcal{F^{-1}}(\hat{x})=Ux
$$
由于频域的乘积等于时域的卷积，因此可以用图傅里叶变换来定义图卷积：
$$
x*y=\mathcal{F^{-1}}(\mathcal{F}(x)\odot\mathcal{F}(y))=U(U^Tx\odot U^Ty)
$$
其中$\odot$即为Hadamard积。

基于二维图像的**空域**卷积范式——模板（卷积核），图网络可以定义一个滤波器作为图网络的卷积核：
$$
g_\theta=diag(U^Tg)
$$
其中$diag()$为对角线化函数。

那么对于图网络，我们也可以使用刚才定义的模板（卷积核）实现卷积操作：
$$
x*g_\theta=Ug_\theta U^Tx
$$


### 嵌入

一种特殊的全连接操作。其目的是数据降维。在NLP领域通常用于词向量（one-hot编码）的降维嵌入，同时还能隐形表示各个词向量之间的关联性。对于一些特殊的数据集也可以使用嵌入的方式对输入数据降维/提取特征。
$$
x_{in}=f_{emb}(x,\theta_{emb})
$$
其中$\theta_{emb}$是参数矩阵。

嵌入层常常设计为全连接层的形式（也有使用多层感知机的）。

### 注意力机制

注意力机制最先被应用于NLP领域的含遗忘门的RNN（长短期记忆，LSTM）编解码器模型中（碍于版面这里不再对LSTM编解码器模型这一模型进行详述）。用于这种sequence-to-sequence模型通过遗忘门的设计，具备了比simple-RNN更好的序列记忆能力。但是对于序列中的每个词，模型一视同仁，没有权重差别。注意力机制引入了序列输入中的权重差别。这项机制最后被用于Transformer模型，并击败了所有的更为复杂的RNN模型。因此可以认为正是注意力机制的出现使得Transformer模型成了为NLP领域state-of-the-art的模型。

如，RNN和Transformer模型中**每个单元**的注意力$c_j$可以被描述为:
$$
c_j=\sum^T_{i=1}\alpha_{ij}h_i
$$
其中，$\alpha$是权重矩阵中的注意力权重值，可以通过反向传播更新；$h_i$是每个单元的状态。

可以见得，引入了注意力机制之后，模型的全局信息都被按权相加被浓缩在注意力值之中。这对于NLP领域的这两大拥有编解码器的模型而言，无疑是凸显重要信息的手段。因为注意力机制能更好的凸显关键信息，因此CV领域也逐渐引入视觉注意力。



### 半监督学习&直推式学习&归纳式学习概述

#### (1) 半监督学习

即原监督学习的训练集中有部分训练样本没有标签，且测试集**不存在**与训练集相同的样本。

训练集$\mathcal{D}$为：
$$
\mathcal{D}=\{X_{tr},y_{tr}|X_{un}\}
$$
同时测试集$X_{te}$满足：
$$
X_{te}\cap X_{un}=\emptyset
$$
监督学习基于平滑假设，即相似或相邻的样本点标签也应该相似。而半监督学习基于以下两种假设：

- **聚类假设**：是指同一聚类中的样本点很可能具有同样的类别标记。 这个假设可以通过另一种等价的方式进行表达，那就是决策边界所穿过的区域应当是数据点较为稀疏的区域，因为如果决策边界穿过数据点较为密集的区域那就很有可能将一个聚类中的样本点分为不同的类别这与聚类假设矛盾。
- **流型假设**是指高维中的数据存在着低维的特性。 、即“处于一个很小的局部邻域内的示例具有相似的性质”。 关于这两者的等价性没有严格的证明，但是高维数据中的数据的低维的特性是通过局部邻域相似性体现的，比如一个在三维空间卷曲的二维纸带，高维的数据全局的距离度量由于维度过高而显得没有区分度，但是如果只考虑局部范围的距离度量，那就会有一定意义。

这两种假设一般是一致的，属于监督学习中平滑假设的在半监督学习中的推广。

#### (2) 直推式学习

同半监督学习，但是测试集本身作为未标记的样本放入训练集进行训练。即测试集$X_{te}=X_{un}$。思路为**直推**，这种学习对目标任务具有较好的适应性，但对于整个样本空间所有可能出现的样本数据，泛化性能可能不如归纳式学习。

#### (3) 归纳式学习

即传统监督学习。使用完全标记的样本作为测试集，训练出一般性的模型，再进行未知数据的预测。思路为**归纳——演绎**，需要通过少量的数据推出普适性的模型来描述整个样本空间。



### 元学习&迁移学习概述

传统的机器学习研究模式是：获取特定任务的大型数据集，然后用这个数据集从头开始训练模型。很明显，这和人类利用以往经验，仅仅通过少量样本就迅速完成学习的情况相差甚远。（人类可以从少量样本中快速学习获取很大的认知能力，但算法模型则需要在大规模的数据集上得到训练才有可能达到人的识别能力）也就是说，人类具有**知识迁移**的能力。据此，提出元学习和迁移学习两种模型：

#### (1) 元学习

元学习即学会如何学习。通常着重于**样本之间的相似性**而非**样本自身的特点**。本质是研究如何让神经网络很好的利用以往的知识，使得能根据新任务的调整自己。

元学习最终所用于分类的数据（不论是support set 还是 query）都不会出现在训练数据中。但模型最终能正确进行分类。本质上是一种**知识迁移**的过程，即网络将**如何寻找相似性**的知识迁移到了新的分类任务中。

以元学习最经典的Siamese网络为例。Siamese网络的损失函数是两个向量之间的相似度。这两个向量的标签可能相同也可能不同。Siamese网络需要迭代出最好的参数矩阵，使得网络学习到如何区分两个样本之间的相似程度。



#### (2) 迁移学习

基础知识：https://zhuanlan.zhihu.com/p/33172587

面经：https://juejin.cn/post/6844903918275657742

就是指利用已经训练好的开源网络模型比如常见的VGG系列、Resnet系列、GoogleNet系列、MobileNet等，利用这些网络和其已经在其他大型数据集上训练好的权重参数，将自己的任务数据集在这些迁移过来的网络上进行训练或者微调。这里迁移是指迁移网络模型和网络预训练的权重。当然，在网络和其权重迁移过来后，还可以在后面加上其他的网络模块，构成新的网络模型。

用开源网络模型进行迁移学习，不仅能大大降低参数迭代的时间，同时还具有更好的泛化性能。使用开源网络本质就是一个**知识迁移**的过程。这都是由于**预初始化的参数网络具有额外的训练数据集信息**的缘故。

##### Domain Adaptation

即领域自适应

https://zhuanlan.zhihu.com/p/50710267

https://www.infoq.cn/article/b40tCBfQ-paDPDz5ZQzl



### Baseline&Benchmark

在试验阶段，我们通常使用一些领域内常见的dataset做benchmark。通过对比这些领域内baseline的效果、（甚至）领域内state-of-the-art的效果与我们论文提出的论文的效果，来凸显我们模型的优越性。

#### (1)Baseline

Baseline通常是某个领域的**最常用模型**，是一系列模型比较中的**参照物**。与Baseline的效果进行对比，能说明我们模型的优化方向是否正确（或者有没有反向优化等）。

#### (2)Benchmark

Benchmark通常是使用某领域内**最常用数据集**对不同方法进行跑分测试的**过程**。能说明我们模型是否比其他模型更优秀。

-----------------------------------------------------

迁移学习相关的Benchmark和其baseline请参考https://github.com/jindongwang/transferlearning/blob/master/data/benchmark.md

---







## Project



## Reference

